#!/bin/bash
# llama.cpp Vision OCR Server - ë¼ì¦ˆë² ë¦¬íŒŒì´ 5 ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

echo "========================================"
echo "  llama.cpp Vision OCR Server Setup"
echo "  For Raspberry Pi 5 (8GB RAM)"
echo "========================================"
echo ""

# í˜„ìž¬ ë””ë ‰í† ë¦¬ í™•ì¸
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
LLAMA_DIR="$SCRIPT_DIR/llama.cpp"
MODELS_DIR="$SCRIPT_DIR/models"
SYNC_DIR="$SCRIPT_DIR/sync_server"

# ë©”ëª¨ë¦¬ ì²´í¬
TOTAL_MEM=$(free -g | awk '/^Mem:/{print $2}')
echo "ì‚¬ìš© ê°€ëŠ¥í•œ RAM: ${TOTAL_MEM}GB"

if [ "$TOTAL_MEM" -lt 7 ]; then
    echo "âš ï¸  ê²½ê³ : 8GB RAM ì´ìƒ ê¶Œìž¥. í˜„ìž¬ ${TOTAL_MEM}GB ê°ì§€ë¨"
    echo "    ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)"
    read -r response
    if [[ ! "$response" =~ ^[Yy]$ ]]; then
        echo "ì„¤ì¹˜ ì·¨ì†Œë¨"
        exit 1
    fi
fi

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
echo "[1/5] ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
sudo apt update
sudo apt install -y build-essential cmake git curl wget
sudo apt install -y python3 python3-pip python3-venv python3-dev
sudo apt install -y libgl1 libglib2.0-0t64 || true

# llama.cpp ë¹Œë“œ
echo "[2/5] llama.cpp ë¹Œë“œ ì¤‘..."
cd "$LLAMA_DIR"
mkdir -p build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release -j$(nproc)

# ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
echo "[3/5] ëª¨ë¸ ë””ë ‰í† ë¦¬ ì„¤ì • ì¤‘..."
mkdir -p "$MODELS_DIR"
echo "ëª¨ë¸ íŒŒì¼ì„ $MODELS_DIR ì— ë°°ì¹˜í•˜ì„¸ìš”"

# Python ê°€ìƒí™˜ê²½ ì„¤ì •
echo "[4/5] Python ê°€ìƒí™˜ê²½ ì„¤ì • ì¤‘..."
cd "$SYNC_DIR"
python3 -m venv venv
source venv/bin/activate
pip install --upgrade pip wheel setuptools
pip install pillow fastapi uvicorn pydantic python-multipart requests

# systemd ì„œë¹„ìŠ¤ íŒŒì¼ ìƒì„±
echo "[5/5] ì„œë¹„ìŠ¤ ì„¤ì • ì¤‘..."

# llama-server ì„œë¹„ìŠ¤
LLAMA_SERVICE="/etc/systemd/system/llama-server.service"
sudo tee $LLAMA_SERVICE > /dev/null <<EOF
[Unit]
Description=llama.cpp Vision LLM Server
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=$LLAMA_DIR/build/bin
ExecStart=$LLAMA_DIR/build/bin/llama-server -m $MODELS_DIR/model.gguf --host 0.0.0.0 --port 408
Restart=always
RestartSec=30
MemoryMax=6G

[Install]
WantedBy=multi-user.target
EOF

# OCR API ì„œë²„ ì„œë¹„ìŠ¤
OCR_API_SERVICE="/etc/systemd/system/receipt-ocr.service"
sudo tee $OCR_API_SERVICE > /dev/null <<EOF
[Unit]
Description=Receipt Ledger OCR API Server
After=network.target llama-server.service
Wants=llama-server.service

[Service]
Type=simple
User=$USER
WorkingDirectory=$SYNC_DIR
Environment="PATH=$SYNC_DIR/venv/bin"
ExecStart=$SYNC_DIR/venv/bin/python -m uvicorn ocr_server:app --host 0.0.0.0 --port 9999
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable receipt-ocr.service

echo ""
echo "========================================"
echo "  ë¹Œë“œ ì™„ë£Œ!"
echo "========================================"
echo ""
echo "ë‹¤ìŒ ë‹¨ê³„:"
echo "1. ëª¨ë¸ íŒŒì¼ì„ $MODELS_DIR/model.gguf ë¡œ ë³µì‚¬"
echo "2. ì„œë¹„ìŠ¤ ì‹œìž‘:"
echo "   sudo systemctl enable llama-server"
echo "   sudo systemctl start llama-server"
echo "   sudo systemctl start receipt-ocr"
echo ""
echo "ðŸ”¹ llama.cpp ì„œë²„: í¬íŠ¸ 408"
echo "ðŸ”¹ OCR API ì„œë²„: í¬íŠ¸ 9999"
echo ""
