"""
OCR Benchmark Runner - llama.cpp Vision Server
With Accuracy Measurement (CER - Character Error Rate)
"""

import os
import time
import json
import glob
import base64
import requests
from tqdm import tqdm
from PIL import Image
import io

# ============== ì„¤ì • ==============
VISION_SERVER_URL = os.environ.get("VISION_SERVER_URL", "http://localhost:408/v1/chat/completions")
VISION_MODEL_NAME = "user-model"
REQUEST_TIMEOUT = 300


def calculate_cer(reference: str, hypothesis: str) -> float:
    """
    Character Error Rate (CER) ê³„ì‚°
    CER = (ì‚½ì… + ì‚­ì œ + ëŒ€ì²´) / ì •ë‹µ ê¸¸ì´
    ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ. 0 = ì™„ë²½, 1 = 100% ì˜¤ë¥˜
    """
    # ê³µë°±/ì¤„ë°”ê¿ˆ ì •ê·œí™”
    ref = reference.replace('\n', ' ').replace('\r', '').strip()
    hyp = hypothesis.replace('\n', ' ').replace('\r', '').strip()
    
    # Levenshtein Distance (Edit Distance)
    m, n = len(ref), len(hyp)
    if m == 0:
        return 1.0 if n > 0 else 0.0
    
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    
    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j
    
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if ref[i-1] == hyp[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])
    
    return dp[m][n] / m


def run_llama_vision(img_path, server_url=VISION_SERVER_URL, model_name=VISION_MODEL_NAME):
    """llama.cpp ì„œë²„ì— Vision ìš”ì²­"""
    start = time.time()
    
    image = Image.open(img_path)
    if image.mode in ('RGBA', 'P'):
        image = image.convert('RGB')
    
    buffered = io.BytesIO()
    image.save(buffered, format="JPEG")
    img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')
    
    prompt = """ì´ ì˜ìˆ˜ì¦ ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ìˆœì„œëŒ€ë¡œ ê·¸ëŒ€ë¡œ ì ì–´ì£¼ì„¸ìš”.
- ìœ„ì—ì„œ ì•„ë˜ë¡œ, ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ ìˆœì„œë¡œ ì½ì–´ì£¼ì„¸ìš”.
- ìˆ«ì, ê°€ê²©, ë‚ ì§œ ë“± ëª¨ë“  ì •ë³´ë¥¼ ë¹ ëœ¨ë¦¬ì§€ ë§ê³  ì ì–´ì£¼ì„¸ìš”.
- í˜•ì‹ì„ ë§ì¶”ë ¤ í•˜ì§€ ë§ê³  ë³´ì´ëŠ” ê·¸ëŒ€ë¡œ ì ì–´ì£¼ì„¸ìš”.
- í’ˆëª©ëª…ì´ ì˜ë ¤ì„œ ë³´ì´ë”ë¼ë„ ë³´ì´ëŠ” ê·¸ëŒ€ë¡œ ì ì–´ì£¼ì„¸ìš” (ì˜ˆ: "ì‹ ë¼ë©´ë©€í‹°" â†’ "ì‹ ë¼ë©´ë©€" ì²˜ëŸ¼ ì˜ë ¤ ë³´ì´ë©´ ê·¸ëŒ€ë¡œ)."""

    payload = {
        "model": model_name,
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_str}"}}
                ]
            }
        ],
        "temperature": 0.1,
        "max_tokens": 2048,
    }
    
    try:
        response = requests.post(server_url, json=payload, timeout=REQUEST_TIMEOUT)
        if response.status_code != 200:
            return {"time": time.time() - start, "text": "", "error": f"Server error: {response.text}"}
        
        result = response.json()
        text = result['choices'][0]['message']['content']
        end = time.time()
        return {"time": end - start, "text": text}
    except Exception as e:
        return {"time": time.time() - start, "text": "", "error": str(e)}


def load_ground_truth(img_path):
    """
    ì´ë¯¸ì§€ì™€ ê°™ì€ ì´ë¦„ì˜ .txt íŒŒì¼ì—ì„œ ì •ë‹µ í…ìŠ¤íŠ¸ ë¡œë“œ
    ì˜ˆ: receipt1.jpg -> receipt1.txt
    """
    base = os.path.splitext(img_path)[0]
    txt_path = base + ".txt"
    if os.path.exists(txt_path):
        with open(txt_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    return None


def main():
    images_dir = "images"
    image_files = glob.glob(os.path.join(images_dir, "*.*"))
    image_files = [f for f in image_files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.webp', '.bmp'))]
    results = {}

    if not image_files:
        print("âŒ No images found in 'images' directory.")
        return

    print(f"ğŸ“· Found {len(image_files)} images.")
    print(f"ğŸ”— Server: {VISION_SERVER_URL}")
    print("\nğŸ’¡ Tip: ì •í™•ë„ ì¸¡ì •í•˜ë ¤ë©´ ì´ë¯¸ì§€ì™€ ê°™ì€ ì´ë¦„ì˜ .txt íŒŒì¼ì— ì •ë‹µ í…ìŠ¤íŠ¸ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")
    print("   ì˜ˆ: receipt.jpg â†’ receipt.txt\n")

    total_time = 0
    total_cer = 0
    cer_count = 0

    for img_path in tqdm(image_files, desc="Processing"):
        filename = os.path.basename(img_path)
        print(f"\n{'='*50}")
        print(f"ğŸ“„ {filename}")
        print('='*50)
        
        result = run_llama_vision(img_path)
        
        if 'error' in result:
            print(f"âŒ Error: {result['error']}")
            results[filename] = result
            continue
        
        total_time += result['time']
        
        # ì •ë‹µ í…ìŠ¤íŠ¸ í™•ì¸
        ground_truth = load_ground_truth(img_path)
        
        if ground_truth:
            cer = calculate_cer(ground_truth, result['text'])
            accuracy = (1 - cer) * 100
            result['cer'] = cer
            result['accuracy'] = accuracy
            result['ground_truth'] = ground_truth
            total_cer += cer
            cer_count += 1
            
            print(f"â±ï¸  Time: {result['time']:.2f}s")
            print(f"âœ… Accuracy: {accuracy:.1f}% (CER: {cer:.3f})")
            print(f"\n[ì •ë‹µ]:\n{ground_truth[:200]}...")
            print(f"\n[ì¶”ì¶œ]:\n{result['text'][:200]}...")
        else:
            print(f"â±ï¸  Time: {result['time']:.2f}s")
            print(f"âš ï¸  No ground truth file found (add {os.path.splitext(filename)[0]}.txt)")
            print(f"\n[ì¶”ì¶œ í…ìŠ¤íŠ¸]:\n{result['text']}")
        
        results[filename] = result

    # Save
    with open("benchmark_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
    
    # Summary
    print("\n" + "="*60)
    print("ğŸ“Š BENCHMARK SUMMARY")
    print("="*60)
    print(f"Total images: {len(image_files)}")
    print(f"Total time: {total_time:.2f}s")
    print(f"Avg time per image: {total_time/len(image_files):.2f}s")
    
    if cer_count > 0:
        avg_cer = total_cer / cer_count
        avg_accuracy = (1 - avg_cer) * 100
        print(f"\nğŸ¯ Average Accuracy: {avg_accuracy:.1f}%")
        print(f"ğŸ“‰ Average CER: {avg_cer:.3f}")
    else:
        print("\nâš ï¸  ì •í™•ë„ë¥¼ ì¸¡ì •í•˜ë ¤ë©´ ì •ë‹µ í…ìŠ¤íŠ¸ íŒŒì¼(.txt)ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    
    print("\nâœ… Results saved to benchmark_results.json")


if __name__ == "__main__":
    main()
